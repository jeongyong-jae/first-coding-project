{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4bfbf9",
   "metadata": {},
   "source": [
    "# ■Introduction\n",
    "   패키지 설치 - Pytorch-lighthing\n",
    "   \n",
    "  ※ Pytorch-lighthing\n",
    "   * Pytorch를 기반한 인공신경망 패키지로 페이스북에서 개발\n",
    "   * 기존 Pytorch보다 코드를 간결하고 쉽게 정리할 수 있어 딥러닝 입문자에게 효과적인 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1021cb",
   "metadata": {},
   "source": [
    "### 1. 모듈 불어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 데이터 패키지\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# 데이터 전처리 패키지\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 기계 학습 모델 패키지\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#!pip install pytorch-lightning # Pytorch-lightning 패키지 다운로드 코드입니다! 없으시면 다운로드 !!\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import functional as FM\n",
    "\n",
    "# 데이터 시각화 패키지\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rc('font', family='Malgun Gothic') # 한글 폰트 설정\n",
    "\n",
    "# 예측 평가 지표 패키지\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 예측 결과 시각화 custom 함수\n",
    "def plot_confusion_matrix(cm, classesm, title, cmap = plt.cm.Blues):\n",
    "    \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(cm, interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j,i,cm[i,j],\n",
    "                horizontalaligment=\"center\",\n",
    "                color=\"red\" if cm[i,j] > thresh else \"black\",fontsize=30)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# CNN filter 시각화 함수\n",
    "def show_filter_filter(parameter_path):\n",
    "    \n",
    "    # parameter load\n",
    "    parameter = torch.load(parameter_path)['state_dict']\n",
    "    \n",
    "    # conv1.weight load\n",
    "    conv1 = parameter['conv1.weight'].detach().cpu().numpy().reshape(1,10,3,3)\n",
    "    for chanel_ind , chanel in  enumerate(conv1):\n",
    "        \n",
    "        fig = plt.figure(figsize=(20,5))\n",
    "        plt.title(f'CONV1 filter - type : {chanel_ind}')\n",
    "        for ind,img in enumerate(chanel):\n",
    "            ax = fig.add_subplot(2,5,ind + 1)\n",
    "            ax.imshow(img, interpolation='nearest', cmap = 'gray')\n",
    "            ax.set_xticks([]), ax.set_yticks([])\n",
    "            \n",
    "            for i,j in itertools.product(range(img.shape[0]), range(img.shape[1])):\n",
    "                plt.text(j,i,round(img[i,j],2),\n",
    "                horizontalalignment=\"center\",color=\"red\",fontsize=12)\n",
    "                \n",
    "    # conv2.weight load\n",
    "    conv2 = parameter['conv2.weight'].detach().cpu().numpy().reshape(20,10,3,3)\n",
    "    for chanel_ind , chanel in  enumerate(conv2):\n",
    "        \n",
    "        fig = plt.figure(figsize=(20,5))\n",
    "        plt.title(f'CONV2 filter - type : {chanel_ind}')\n",
    "        for ind,img in enumerate(chanel):\n",
    "            ax = fig.add_subplot(2,5,ind + 1)\n",
    "            ax.imshow(img, interpolation='nearest', cmap = 'gray')\n",
    "            ax.set_xticks([]), ax.set_yticks([])\n",
    "            \n",
    "            for i,j in itertools.product(range(img.shape[0]), range(img.shape[1])):\n",
    "                plt.text(j,i,round(img[i,j],2),\n",
    "                horizontalalignment=\"center\",color=\"red\",fontsize=12)\n",
    "    \n",
    "    # conv3.weight load\n",
    "    conv3 = parameter['conv3.weight'].detach().cpu().numpy().reshape(40,20,3,3)\n",
    "    for chanel_ind , chanel in  enumerate(conv3):\n",
    "        \n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "        plt.title(f'CONV3 filter - type : {chanel_ind}')\n",
    "        for ind,img in enumerate(chanel):\n",
    "            ax = fig.add_subplot(4,5,ind + 1)\n",
    "            ax.imshow(img, interpolation='nearest', cmap = 'gray')\n",
    "            ax.set_xticks([]), ax.set_yticks([])\n",
    "            \n",
    "            for i,j in itertools.product(range(img.shape[0]), range(img.shape[1])):\n",
    "                plt.text(j,i,round(img[i,j],2),\n",
    "                horizontalalignment=\"center\",color=\"red\",fontsize=12)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 기타\n",
    "import warnings, itertools, time\n",
    "warnings. filterwarnings(action='ignore')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b389ba0",
   "metadata": {},
   "source": [
    "### 2. 데이터 불러오고 로더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2866f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST 데이터를 저장할 directory\n",
    "download_root = './'\n",
    "\n",
    "# 데이터 scaling을 위한 요소\n",
    "fasion_mnist_transform = transfroms.Compose([\n",
    "    transforms.ToTensor(),#데이터 형태를 Pytorch에 적합한 형태로 바꾸어 줌\n",
    "    transforms.Normalize((0.0,),(1.0,)) #feature들의 평균과 표준편차를 0과 1로 scaling 해줌 \n",
    "])\n",
    "\n",
    "#FashionMNIST 훈련 데이터셋 다운로드\n",
    "dataset = FashionMNIST(download_root, transform=fasion_mnist_transform, trains = True, download = True)\n",
    "\n",
    "# 훈련 데이터셋을 50000개(학습용) / 10000개(검증용)으로 분리\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [50000,10000])\n",
    "\n",
    "# 테스트 데이터셋 다운로드\n",
    "test_dataset = FashionMNIST(download_root, transform = fasion_mnist_transform, train =False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 100개 단위의 미니 배치로 구분지어 인공신경망 학습\n",
    "train_loader = DataLoader(train_dataset,batch_size=100)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=100)\n",
    "test_loader = DataLoader(test_dataset,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4a28c",
   "metadata": {},
   "source": [
    "### 3. Pytorch-lightning을 이용한 CNN 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20574c1f",
   "metadata": {},
   "source": [
    "#### 3.1 모델 구축\n",
    "     CNN\n",
    "# ■CNN 모델 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ed14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(pl.LightningModule):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolution\n",
    "        self.conv1 = nn.Conv2d(1,10,(3,3))\n",
    "        self.conv2 = nn.Conv2d(10,20,(3,3))\n",
    "        self.conv3 = nn.Conv2d(20,40,(3,3))\n",
    "        \n",
    "        # pooling layer\n",
    "        self.max_pool = nn.MaxPool2d((2,2),2)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout_1 = nn.Dropout(p=0.3)\n",
    "        self.dropout_2 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        # activation function\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slop=0.1,inplace = True)\n",
    "        \n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(nn.Linear(500,300),\n",
    "                                       self.relu,\n",
    "                                       self.dropout_1,\n",
    "                                       nn.Linear(300,100),\n",
    "                                       self.lrelu,\n",
    "                                       self.dropout_2,\n",
    "                                       nn.Linear(100,class_num))\n",
    "        \n",
    "        # for logging\n",
    "        self.train_loss_ = []\n",
    "        self.train_acc_ = []\n",
    "        self.val_loss_ = []\n",
    "        self.val_acc_ = []\n",
    "        \n",
    "        # for convenient\n",
    "        self.class_num = class_num\n",
    "        \n",
    "    def forward(sefl,x): # x is input data\n",
    "        \n",
    "        # x -> convolution  -> relu -> max_pool -> dropout\n",
    "        x = self. conv1(x)\n",
    "        x = self. relu(x)\n",
    "        x = self. max_pool(x)\n",
    "        x = self. dropout_1(x)\n",
    "        \n",
    "        # .. -> convolution -> relu -> max_pool -> dropout\n",
    "         x = self. conv2(x)\n",
    "        x = self. lrelu(x)\n",
    "        x = self. max_pool(x)\n",
    "        x = self. dropout_2(x)\n",
    "        \n",
    "        # .. => classifier -> logit\n",
    "        flatten = x.view(x.size(0),-1)\n",
    "        logit = self.classifier(flatten)\n",
    "        \n",
    "        return logit\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters() , lr = 0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat =self(x)\n",
    "        y_hat_prob = F.softmax(y_hat,dim=1)\n",
    "        val_accuracy = FM.accuracy(y_hat_prob,y)\n",
    "        val_loss = F.cross_entropy(y_hat,y)\n",
    "        logs = {'val accuracy': val accuracy, 'val loss': val loss}\n",
    "        result = {'loss':val_loss,'log':logs, 'val_acc' : val_accuracy}\n",
    "        return result\n",
    "    \n",
    "    def training_epoch_end(self,result):\n",
    "        avg_loss = torch.stack([x['loss'] for x in result]).mean()\n",
    "        avg_acc = torch.stack([x['train_acc'] for x in result]).mean()\n",
    "        epoch_result = {'loss':avg_loss, 'train_acc':avg_sec}\n",
    "        self.train_loss_.append(avg_loss.cpu().numpy(),item())\n",
    "        self.train_acc_.append(avg_acc.cpu().numpy(),item())\n",
    "        return epoch_result\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat_prob = F.softmax(y_hat,dim=1)\n",
    "        val_accuracy = FM.accuracy(y_hat_prob,y)\n",
    "        val_loss = F.cross_entropy(y_hat,y)\n",
    "        logs = {'val accuracy': val accuracy, 'val loss': val loss}\n",
    "        result = {'loss':val_loss,'log':logs, 'val_acc' : val_accuracy}\n",
    "        return result\n",
    "    \n",
    "    def validation_epoch_end(self,result):\n",
    "        avg_loss = torch.stack([x['loss'] for x in result]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in result]).mean()\n",
    "        epoch_result = {'loss':avg_loss, 'acc':avg_sec}\n",
    "        self.val_loss_.append(avg_loss.cpu().numpy(),item())\n",
    "        self.val_acc_.append(avg_acc.cpu().numpy(),item())\n",
    "        return epoch_result\n",
    "    \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat_prob = F.softmax(y_hat,dim=1)\n",
    "        result = {'predicted':y_hat_prob,'target':y}\n",
    "        return result\n",
    "    \n",
    "    def test_epoch_end(self,result):\n",
    "        \n",
    "        predicted = torch.stack([x['predicted'] for x in result])\n",
    "        predicted = predicted.view(-1,self.class_num)\n",
    "        target = torch.stack([x['target'] for x in result])\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        self.test_predicted = predicted.detach().cpu().numpy()\n",
    "        self.test_target = target.detach().cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573db353",
   "metadata": {},
   "source": [
    "### 3.2 모델 학습\n",
    "\n",
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # 모델 학습 시작 시간 저장\n",
    "cnn = CNN(class_num=10)\n",
    "ealry_stop = EarlyStopping(monitor= 'val_accuracy'.patience=5,verbose=True,mode='max') #Early stopping\n",
    "checkpoint = ModelCheckpoint(filename='./CNN-{epoch}-{val_accuracy}',monitor='val_accuracy'.mode='max',save_top_k=1) #Mode\n",
    "trainer = pl.Trainer(callbacks=[ealry_stop,checkpoint],gpus=0)\n",
    "trainer.fit(cnn,train_loader,valid_loader)\n",
    "print(\"time :\", time.time() - start) # 학습 소요 시간 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed88f9",
   "metadata": {},
   "source": [
    "#### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # 모델 학습 시작 시간 저장\n",
    "dnn = DNN(class_num=10)\n",
    "ealry_stop = EarlyStopping(monitor= 'val_accuracy'.patience=5,verbose=True,mode='max') #Early stopping\n",
    "checkpoint = ModelCheckpoint(filename='./DNN-{epoch}-{val_accuracy}',monitor='val_accuracy'.mode='max',save_top_k=1) #Mode\n",
    "trainer = pl.Trainer(callbacks=[ealry_stop,checkpoint],gpus=0)\n",
    "trainer.fit(dnn,train_loader,valid_loader)\n",
    "print(\"time :\", time.time() - start) # 학습 소요 시간 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380af2f",
   "metadata": {},
   "source": [
    "### 3.3 학습 현황 확인 및 예측 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63879e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN logs\n",
    "cnn_train_loss =cnn.train_loss_\n",
    "cnn_train_acc =cnn.train_acc_\n",
    "cnn_valid_loss =cnn.val_loss_\n",
    "cnn_valid_acc =cnn.val_acc_\n",
    "\n",
    "# DNN logs\n",
    "dnn_train_loss =dnn.train_loss_\n",
    "dnn_train_acc =dnn.train_acc_\n",
    "dnn_valid_loss =dnn.val_loss_\n",
    "dnn_valid_acc =dnn.val_acc_\n",
    "\n",
    "# 그래프 배경화면 만들기\n",
    "f, axs = plt.subplots(4,1,figsize = (20,30))\n",
    "\n",
    "# 학습로스 변동 관찰하기\n",
    "axs[0].plot(cnn_train_loss,label='CNN 학습 로스')\n",
    "axs[0].plot(dnn_train_loss,label='DNN 학습 로스')\n",
    "axs[0].legend()\n",
    "\n",
    "# 학습 정확도 변동 관찰하기\n",
    "axs[1].plot(cnn_train_acc,label='CNN 학습 정확도')\n",
    "axs[1].plot(dnn_train_acc,label='DNN 학습 정확도')\n",
    "axs[1].legend()\n",
    "\n",
    "# 검증용 데이터에 대한 Accuracy 변동 관찰하기\n",
    "axs[2].plot(cnn_valid_acc,label='CNN검증 정확도')\n",
    "axs[2].scatter(np.argmax(cnn_valid_acc),np.max(cnn_valid_acc),s=100,facecolors='none',edgecolors='r',\n",
    "              label='CNN 최적의 학습 체크포인트(=학습 중단점)')\n",
    "axs[2].plot(dnn_train_acc,label='DNN 학습 정확도')\n",
    "axs[2].scatter(np.argmax(dnn_valid_acc),np.max(dnn_valid_acc),s=100,facecolors='none',edgecolors='r',\n",
    "              label='DNN 최적의 학습 체크포인트(=학습 중단점)')\n",
    "axs[2].legend()\n",
    "\n",
    "# 검증용 데이터에 대한 loss 변동 관찰하기\n",
    "axs[3].plot(cnn_valid_loss,label='CNN 검증 로스')\n",
    "axs[3].plot(dnn_valid_loss,label='DNN 검증 로스')\n",
    "axs[3].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e78bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Accuracy 확인하기\n",
    "\n",
    "trainer.test(model=cnn, test_dataloaders=test_loader) #test data에 대한 예측 수행\n",
    "trainer.test(model=dnn, test_dataloaders=test_loader) #test data에 대한 예측 수행\n",
    "\n",
    "cnn_test_accuracy = accuracy_score(y_pred=cnn.test_predicted.argmax(1),y_true=cnn.test_target)\n",
    "dnn_test_accuracy = accuracy_score(y_pred=dnn.test_predicted.argmax(1),y_true=dnn.test_target)\n",
    "print(f\"CNN 분류 정확도: {cnn_test_accuracy:3f}\",f\"DNN 분류 정확도 : {dnn_test_accuracy:.3f}\",sep=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a37bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_ = confusion_matrix(y_pred=cnn.test_predicted.argmax(1),y_true=cnn.test_target)\n",
    "plot_confusion_matrix(cm = confusion_matrix_, classes=dataset.classes,title='CNN을 이용한 Fashion MNIST 예측 결과')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_ = confusion_matrix(y_pred=dnn.test_predicted.argmax(1),y_true=dnn.test_target)\n",
    "plot_confusion_matrix(cm = confusion_matrix_, classes=dataset.classes,title='DNN을 이용한 Fashion MNIST 예측 결과')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_visualization_test_x = test_dataset.data.numpy()[:100]\n",
    "for_visualization_test_y = test_dataset.targets.numpy()[:100]\n",
    "\n",
    "f , axs = plt.subplots(10,10,figsize =(20,20))\n",
    "plt.subplots_adjust(hspace= 0.8)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        \n",
    "        x_data = for_visualization_test_x[10*i+j]\n",
    "        y_data = for_visualization_test_y[10*i+j]\n",
    "        real_class = dataset.classes[y_data]\n",
    "        predicted_class = datasset.classes[cnn.test_predicted[10*i+j].argmax()]\n",
    "        predicted_prob = cnn.test_predicted[10*i+j].max()\n",
    "        \n",
    "        # x, y 축의 지점 표시를 안함\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "        \n",
    "        # subplot의 제목을 i번째 결과에 해당하는 숫자로 설정\n",
    "        axs[i,j].set_title(f\"실제 class:{real_class}\\n예측 class:{predicted_class}\\nProbability:{predicted_prob:.2f}\")\n",
    "        \n",
    "        # 입력으로 사용한 i번째 테스트 이미지를 28x28로 재배열하고\n",
    "        # 이 2차원 배열을 그레이스케일 이미지로 출력\n",
    "        axs[i,j].imshow(x_data.reshape((28,28)),cmap=plt.cm.gray_r)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_path = \"./lightning_logs/version_11/checkpoints/CNN-epoch=34-val_accuracy=0.9134000539779663.ckpt\"\n",
    "show_filter_filter(parameter_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
